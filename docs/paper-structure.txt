Title
TRAM-MP: Multi-Person Trajectory and Motion Reconstruction from Videos in the Wild


Abstract
We present TRAM-MP, a feed-forward method for reconstructing global trajectories and body motions of multiple people from monocular videos with moving cameras. Building on TRAM's scene-centric philosophy, we extend it to multi-person scenarios through three key contributions: (1) we replace DROID-SLAM with VGGT's feed-forward transformer, achieving 20-30Ã— speedup in camera estimation while maintaining metric-scale accuracy; (2) we introduce depth consistency constraints that leverage VGGT's scene depth predictions to provide stronger scale estimation than motion-prior-based methods, particularly for non-MoCap motions like parkour and skateboarding; (3) we propose multi-person scale constraints that aggregate motion likelihoods across all individuals, providing N independent signals for improved scale recovery. Our two-stage approach processes each person independently through VIMO in the camera frame, then transforms all to a shared world coordinate system using the recovered metric-scale cameras. TRAM-MP achieves state-of-the-art accuracy on multi-person benchmarks with 60% error reduction in root trajectory estimation (RTE: 1.4% vs 10.2%) compared to SLAHMR, while operating in feed-forward mode at ~12 seconds for 100 frames with 10 people. Unlike prior work relying on MoCap-trained motion priors, our scene-centric approach generalizes to diverse human activities in challenging real-world videos. Code and models will be publicly available.